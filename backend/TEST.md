# Guide de Test BoursaVision üß™

> **Version**: 1.0 | **Date**: D√©cembre 2024  
> **Architecture**: Clean Architecture + CQRS + Dependency Injection  
> **Framework**: pytest 8.4.1 + dependency-injector 4.48.1

## üìã Table des Mati√®res

- [üéØ Philosophie de Test](#-philosophie-de-test)
- [üèõÔ∏è Architecture de Test](#Ô∏è-architecture-de-test)
- [üîß Configuration et Outils](#-configuration-et-outils)
- [üìö Types de Tests](#-types-de-tests)
- [üé≠ Fixtures et Mocks](#-fixtures-et-mocks)
- [üíâ Dependency Injection pour Tests](#-dependency-injection-pour-tests)
- [üèÉ‚Äç‚ôÇÔ∏è Strat√©gies d'Ex√©cution](#Ô∏è-strat√©gies-dex√©cution)
- [üìä Coverage et M√©triques](#-coverage-et-m√©triques)
- [üõ†Ô∏è Conventions et Standards](#Ô∏è-conventions-et-standards)
- [üîÑ Migration des Tests Existants](#-migration-des-tests-existants)

---

## üéØ Philosophie de Test

### Principes Fondamentaux

**1. Test-Driven Quality (TDQ)**
- Chaque test doit avoir un **objectif m√©tier clair**
- Pas de tests "pour la forme" - chaque test doit apporter de la valeur
- Focus sur la **behaviour verification** plut√¥t que l'impl√©mentation

**2. Clean Test Architecture**
- Les tests suivent la m√™me Clean Architecture que l'application
- S√©paration claire : Unit ‚Üí Integration ‚Üí E2E
- Dependency Injection native pour l'isolation

**3. Fast Feedback Loop**
- Tests unitaires < 100ms par test
- Suite compl√®te < 2 minutes
- Parall√©lisation intelligente avec pytest-xdist

**4. Reliability First**
- Tests d√©terministes (pas de flaky tests)
- Isolation compl√®te entre tests
- Gestion explicite des d√©pendances externes

---

## üèõÔ∏è Architecture de Test

### Structure des Tests

```
tests/
‚îú‚îÄ‚îÄ conftest.py                 # Configuration globale pytest
‚îú‚îÄ‚îÄ fixtures/                   # Fixtures r√©utilisables
‚îÇ   ‚îú‚îÄ‚îÄ container_fixtures.py   # DI containers pour tests
‚îÇ   ‚îú‚îÄ‚îÄ data_fixtures.py        # Donn√©es de test
‚îÇ   ‚îî‚îÄ‚îÄ mock_fixtures.py        # Mocks et stubs
‚îú‚îÄ‚îÄ factories/                  # Factory Boy patterns
‚îÇ   ‚îú‚îÄ‚îÄ portfolio_factory.py
‚îÇ   ‚îú‚îÄ‚îÄ investment_factory.py
‚îÇ   ‚îî‚îÄ‚îÄ user_factory.py
‚îú‚îÄ‚îÄ unit/                       # Tests unitaires (< 100ms)
‚îÇ   ‚îú‚îÄ‚îÄ domain/                 # Entit√©s et value objects
‚îÇ   ‚îú‚îÄ‚îÄ application/            # Use cases et services
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/         # Adapters et repositories
‚îÇ   ‚îî‚îÄ‚îÄ presentation/           # Controllers et API
‚îú‚îÄ‚îÄ integration/                # Tests d'int√©gration
‚îÇ   ‚îú‚îÄ‚îÄ database/              # Persistence layer
‚îÇ   ‚îú‚îÄ‚îÄ external_apis/         # Services externes
‚îÇ   ‚îî‚îÄ‚îÄ messaging/             # Event handling
‚îú‚îÄ‚îÄ e2e/                       # Tests end-to-end
‚îÇ   ‚îú‚îÄ‚îÄ api/                   # Tests API complets
‚îÇ   ‚îî‚îÄ‚îÄ scenarios/             # User scenarios
‚îî‚îÄ‚îÄ performance/               # Tests de performance
    ‚îú‚îÄ‚îÄ load/                  # Tests de charge
    ‚îî‚îÄ‚îÄ stress/                # Tests de stress
```

### Mapping avec Clean Architecture

| Couche Architecture | Type Test | Focus | Dur√©e Cible |
|-------------------|-----------|-------|-------------|
| **Domain** | Unit | Logique m√©tier pure | < 50ms |
| **Application** | Unit + Integration | Use cases, CQRS | < 100ms |
| **Infrastructure** | Integration | Repositories, APIs | < 500ms |
| **Presentation** | Integration + E2E | Controllers, validation | < 1s |

---

## üîß Configuration et Outils

### Pytest 8.4.1 - Nouvelles Fonctionnalit√©s

**1. Async/Await Native Support**
```python
# Nouveau dans pytest 8.4.1
@pytest.mark.asyncio
async def test_async_use_case(container_fixture):
    # Pas besoin de pytest-asyncio additionnel
    use_case = await container_fixture.application.get_portfolio_use_case()
    result = await use_case.execute(portfolio_id="123")
    assert result.success
```

**2. Improved Fixture Scoping**
```python
# Scoping plus granulaire
@pytest.fixture(scope="session")
def test_container():
    """Container principal pour tous les tests."""
    pass

@pytest.fixture(scope="class") 
def repository_container():
    """Container repository r√©initialis√© par classe."""
    pass

@pytest.fixture(scope="function")
def clean_database():
    """Base propre pour chaque test."""
    pass
```

**3. Enhanced Parametrize**
```python
# Parametrize avec dependency injection
@pytest.mark.parametrize("use_case", [
    pytest.param("create_portfolio", id="create"),
    pytest.param("update_portfolio", id="update"),
], indirect=True)
def test_portfolio_use_cases(use_case, sample_data):
    result = use_case.execute(sample_data)
    assert result.is_valid()
```

### Configuration pytest.ini Optimis√©e

```ini
[pytest]
# Chemins et d√©couverte
testpaths = tests
pythonpath = src
python_files = test_*.py *_test.py
python_classes = Test* *Tests
python_functions = test_* should_*

# Ex√©cution asynchrone native
asyncio_mode = auto
asyncio_default_fixture_loop_scope = session

# Performance et parall√©lisation  
addopts = 
    -v
    --tb=short
    --strict-config
    --maxfail=5
    --no-header
    -x  # Stop √† la premi√®re erreur en d√©veloppement
    --lf  # Run last failed d'abord
    --ff  # Failed first
    --durations=10  # Montre les 10 tests les plus lents

# Markers obligatoires
markers =
    unit: Tests unitaires (< 100ms)
    integration: Tests d'int√©gration (< 1s)
    e2e: Tests end-to-end (< 10s)
    performance: Tests de performance
    slow: Tests lents (> 1s)
    database: Tests n√©cessitant une base de donn√©es
    external: Tests avec services externes
    container: Tests utilisant le DI container

# Filtres d'avertissement
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    error::UserWarning  # Transforme UserWarning en erreur
```

---

## üìö Types de Tests

### 1. Tests Unitaires (Unit)

**Objectif**: Tester la logique m√©tier en isolation  
**Dur√©e**: < 100ms par test  
**Scope**: Fonction/M√©thode individuelle

```python
@pytest.mark.unit
class TestPortfolioDomain:
    """Tests de la logique m√©tier Portfolio."""
    
    def test_portfolio_creation_with_valid_data(self, portfolio_factory):
        """Should create portfolio when data is valid."""
        # Arrange
        data = portfolio_factory.build_data()
        
        # Act
        portfolio = Portfolio.create(data)
        
        # Assert
        assert portfolio.is_valid()
        assert portfolio.name == data["name"]
        assert len(portfolio.investments) == 0
    
    def test_add_investment_increases_total_value(self, portfolio, investment):
        """Should increase portfolio value when investment is added."""
        # Arrange
        initial_value = portfolio.total_value
        
        # Act
        portfolio.add_investment(investment)
        
        # Assert
        assert portfolio.total_value > initial_value
        assert investment in portfolio.investments
```

### 2. Tests d'Int√©gration (Integration)

**Objectif**: Tester l'interaction entre composants  
**Dur√©e**: < 1s par test  
**Scope**: Interaction entre couches

```python
@pytest.mark.integration
@pytest.mark.database
class TestPortfolioRepository:
    """Tests d'int√©gration pour PortfolioRepository."""
    
    async def test_save_portfolio_persists_to_database(
        self, 
        test_container,
        clean_database,
        portfolio_factory
    ):
        """Should persist portfolio to database when saved."""
        # Arrange
        repository = test_container.repositories.portfolio_repository()
        portfolio = portfolio_factory.create()
        
        # Act
        saved_portfolio = await repository.save(portfolio)
        
        # Assert
        assert saved_portfolio.id is not None
        
        # Verify persistence
        found_portfolio = await repository.find_by_id(saved_portfolio.id)
        assert found_portfolio == saved_portfolio
```

### 3. Tests End-to-End (E2E)

**Objectif**: Tester les sc√©narios utilisateur complets  
**Dur√©e**: < 10s par test  
**Scope**: Application compl√®te

```python
@pytest.mark.e2e
@pytest.mark.asyncio
class TestPortfolioManagementScenarios:
    """Tests E2E pour la gestion de portefeuilles."""
    
    async def test_complete_portfolio_lifecycle(
        self,
        test_client,
        authenticated_user_headers
    ):
        """Should handle complete portfolio lifecycle."""
        # Arrange
        portfolio_data = {
            "name": "Tech Portfolio", 
            "description": "Technology investments"
        }
        
        # Act 1: Create Portfolio
        response = await test_client.post(
            "/api/v1/portfolios",
            json=portfolio_data,
            headers=authenticated_user_headers
        )
        assert response.status_code == 201
        portfolio = response.json()
        
        # Act 2: Add Investment
        investment_data = {
            "symbol": "AAPL",
            "quantity": 10,
            "purchase_price": 150.0
        }
        response = await test_client.post(
            f"/api/v1/portfolios/{portfolio['id']}/investments",
            json=investment_data,
            headers=authenticated_user_headers
        )
        assert response.status_code == 201
        
        # Act 3: Retrieve Updated Portfolio
        response = await test_client.get(
            f"/api/v1/portfolios/{portfolio['id']}",
            headers=authenticated_user_headers
        )
        assert response.status_code == 200
        
        # Assert
        updated_portfolio = response.json()
        assert len(updated_portfolio["investments"]) == 1
        assert updated_portfolio["total_value"] > 0
```

---

## üé≠ Fixtures et Mocks

### Fixtures Stratifi√©es par Couche

```python
# tests/fixtures/container_fixtures.py
"""Fixtures pour l'injection de d√©pendances."""

import pytest
from boursa_vision.containers.main_clean import MainContainer


@pytest.fixture(scope="session")
def base_test_container():
    """Container de base pour tous les tests."""
    container = MainContainer()
    
    # Override pour l'environnement de test
    container.core.config.override({
        "environment": "testing",
        "database": {"url": "sqlite:///:memory:"},
        "redis": {"url": "redis://localhost:6379/15"},
    })
    
    return container


@pytest.fixture(scope="class")
def test_container(base_test_container):
    """Container avec overrides pour tests d'int√©gration."""
    container = base_test_container
    
    # Mock des services externes
    container.infrastructure.yfinance_client.override(MockYFinanceClient())
    container.infrastructure.email_service.override(MockEmailService())
    
    yield container
    
    # Cleanup
    container.infrastructure.yfinance_client.reset_override()
    container.infrastructure.email_service.reset_override()


@pytest.fixture
def unit_container(base_test_container):
    """Container pour tests unitaires avec mocks complets."""
    container = base_test_container
    
    # Override tous les repositories avec des mocks
    from unittest.mock import AsyncMock
    
    mock_portfolio_repo = AsyncMock()
    mock_investment_repo = AsyncMock() 
    mock_user_repo = AsyncMock()
    
    container.repositories.portfolio_repository.override(mock_portfolio_repo)
    container.repositories.investment_repository.override(mock_investment_repo)
    container.repositories.user_repository.override(mock_user_repo)
    
    yield container
    
    # Cleanup
    container.repositories.portfolio_repository.reset_override()
    container.repositories.investment_repository.reset_override()
    container.repositories.user_repository.reset_override()
```

### Factory Pattern avec Dependency Injection

```python
# tests/factories/portfolio_factory.py
"""Factory pour les objets Portfolio."""

import factory
from factory import fuzzy
from boursa_vision.domain.entities.portfolio import Portfolio


class PortfolioFactory(factory.Factory):
    """Factory pour cr√©er des objets Portfolio."""
    
    class Meta:
        model = Portfolio
    
    name = factory.Sequence(lambda n: f"Portfolio {n}")
    description = factory.Faker("text", max_nb_chars=200)
    user_id = factory.Faker("uuid4")
    created_at = factory.Faker("date_time_this_year")
    
    @classmethod
    def build_data(cls, **kwargs):
        """Construit uniquement les donn√©es sans cr√©er l'objet."""
        return factory.build(dict, FACTORY_FOR=cls._meta.model, **kwargs)
    
    @classmethod
    def create_with_investments(cls, investment_count=3, **kwargs):
        """Cr√©e un portfolio avec des investissements."""
        portfolio = cls.create(**kwargs)
        
        for _ in range(investment_count):
            investment = InvestmentFactory.create()
            portfolio.add_investment(investment)
            
        return portfolio
```

### Mocks Intelligents avec Dependency Injector

```python
# tests/fixtures/mock_fixtures.py
"""Mocks avanc√©s pour les services externes."""

from unittest.mock import AsyncMock, MagicMock
import pytest


class MockYFinanceClient:
    """Mock intelligent du client YFinance."""
    
    def __init__(self):
        self.call_count = 0
        self._market_data = {
            "AAPL": {"price": 150.0, "volume": 1000000},
            "GOOGL": {"price": 2500.0, "volume": 500000},
        }
    
    async def get_stock_price(self, symbol: str) -> float:
        """Mock du prix d'action."""
        self.call_count += 1
        return self._market_data.get(symbol, {"price": 100.0})["price"]
    
    def set_stock_price(self, symbol: str, price: float):
        """Permet de configurer les prix pour les tests."""
        if symbol not in self._market_data:
            self._market_data[symbol] = {"volume": 1000}
        self._market_data[symbol]["price"] = price


@pytest.fixture
def mock_yfinance():
    """Fixture pour le mock YFinance."""
    return MockYFinanceClient()


@pytest.fixture
def spy_email_service():
    """Spy pour v√©rifier l'envoi d'emails."""
    spy = MagicMock()
    spy.send_email = AsyncMock(return_value=True)
    spy.send_bulk_email = AsyncMock(return_value=True)
    return spy
```

---

## üíâ Dependency Injection pour Tests

### Strat√©gie d'Override par Couche

```python
# tests/conftest.py
"""Configuration pytest avec DI avanc√©e."""

import pytest
from boursa_vision.containers.main_clean import MainContainer


class TestEnvironmentManager:
    """Gestionnaire d'environnement de test."""
    
    def __init__(self, container: MainContainer):
        self.container = container
        self.overrides = {}
    
    def override_for_unit_tests(self):
        """Configure le container pour les tests unitaires."""
        # Mock tous les I/O
        self.override_database_layer()
        self.override_external_services()
        self.override_infrastructure()
        
    def override_for_integration_tests(self):
        """Configure le container pour les tests d'int√©gration."""
        # Garde la vraie DB (en m√©moire), mock les services externes
        self.override_external_services()
        
    def override_database_layer(self):
        """Override la couche base de donn√©es."""
        from unittest.mock import AsyncMock
        
        # Repositories en mode mock
        self.container.repositories.portfolio_repository.override(
            AsyncMock(spec=PortfolioRepository)
        )
        
    def override_external_services(self):
        """Override les services externes."""
        self.container.infrastructure.yfinance_client.override(
            MockYFinanceClient()
        )
        
    def cleanup(self):
        """Nettoie tous les overrides."""
        for override in self.overrides:
            override.reset_override()


@pytest.fixture(scope="session")
def test_environment():
    """Environment de test global."""
    container = MainContainer()
    env_manager = TestEnvironmentManager(container)
    return env_manager


@pytest.fixture
def unit_test_environment(test_environment):
    """Environment pour tests unitaires."""
    test_environment.override_for_unit_tests()
    yield test_environment.container
    test_environment.cleanup()


@pytest.fixture  
def integration_test_environment(test_environment):
    """Environment pour tests d'int√©gration."""
    test_environment.override_for_integration_tests()
    yield test_environment.container
    test_environment.cleanup()
```

### Fixtures Param√©tr√©es avec DI

```python
@pytest.fixture(params=["memory", "redis", "file"])
def cache_service(request, test_container):
    """Fixture param√©tr√©e pour diff√©rents types de cache."""
    cache_type = request.param
    
    if cache_type == "memory":
        service = MemoryCacheService()
    elif cache_type == "redis": 
        service = RedisCacheService(url="redis://localhost:6379/15")
    else:
        service = FileCacheService(path="/tmp/test_cache")
    
    # Override dans le container
    test_container.infrastructure.cache_service.override(service)
    
    yield service
    
    # Cleanup
    test_container.infrastructure.cache_service.reset_override()
    if hasattr(service, 'cleanup'):
        service.cleanup()


@pytest.mark.parametrize("cache_service", ["memory", "redis"], indirect=True)
def test_portfolio_caching_with_different_backends(cache_service, portfolio):
    """Test le cache avec diff√©rents backends."""
    # Le test utilise automatiquement le service overrid√©
    cache_service.set("portfolio_123", portfolio)
    cached = cache_service.get("portfolio_123")
    
    assert cached == portfolio
```

---

## üèÉ‚Äç‚ôÇÔ∏è Strat√©gies d'Ex√©cution

### Profils d'Ex√©cution par Environnement

```bash
# D√©veloppement - Tests rapides seulement
pytest -m "unit and fast" --maxfail=1 -x

# CI/CD - Suite compl√®te
pytest -m "not slow" --cov=src --cov-report=xml --junitxml=results.xml

# Before Push - Tests critiques
pytest -m "unit or (integration and not external)" --durations=10

# Release - Tests complets avec performance
pytest --cov=src --cov-report=html --cov-fail-under=90 -n auto
```

### Configuration par Markers

```python
# pytest.ini markers avanc√©s
markers =
    # Vitesse d'ex√©cution
    fast: Tests < 100ms
    medium: Tests < 1s  
    slow: Tests > 1s
    
    # Types de test
    unit: Tests unitaires
    integration: Tests d'int√©gration
    e2e: Tests end-to-end
    performance: Tests de performance
    
    # D√©pendances
    database: N√©cessite une base de donn√©es
    redis: N√©cessite Redis
    external: Appelle des services externes
    network: N√©cessite une connexion r√©seau
    
    # Domaines m√©tier
    portfolio: Tests li√©s aux portfolios
    investment: Tests li√©s aux investissements
    user: Tests li√©s aux utilisateurs
    auth: Tests d'authentification
    
    # Criticit√©
    critical: Tests critiques (ne peuvent pas √©chouer)
    regression: Tests de non-r√©gression
    smoke: Tests de smoke
```

### Parall√©lisation Intelligente

```python
# conftest.py - Configuration pour pytest-xdist
def pytest_collection_modifyitems(config, items):
    """Optimise la r√©partition des tests pour la parall√©lisation."""
    
    # Grouper les tests par type pour √©viter les conflits
    for item in items:
        # Tests database dans le m√™me worker
        if "database" in item.keywords:
            item.add_marker(pytest.mark.xdist_group(name="database"))
            
        # Tests externes s√©par√©s
        if "external" in item.keywords:
            item.add_marker(pytest.mark.xdist_group(name="external"))
            
        # Tests unitaires r√©partis librement
        if "unit" in item.keywords:
            item.add_marker(pytest.mark.xdist_group(name=f"unit_{hash(item.nodeid) % 4}"))
```

---

## üìä Coverage et M√©triques

### Configuration Coverage Avanc√©e

```ini
# .coveragerc
[run]
source = src/
omit = 
    */tests/*
    */migrations/*
    */conftest.py
    */factories.py
    */__pycache__/*

[report]
# Seuils de coverage par type
fail_under = 85
show_missing = true
skip_covered = false

# Exclude des lignes sp√©cifiques
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

[html]
directory = htmlcov
title = BoursaVision Test Coverage

[xml]
output = coverage.xml
```

### M√©triques par Couche d'Architecture

```python
# tests/test_coverage_requirements.py
"""Tests pour v√©rifier les exigences de coverage par couche."""

import pytest
import coverage
from pathlib import Path


class TestCoverageRequirements:
    """V√©rifie que chaque couche respecte ses exigences de coverage."""
    
    def test_domain_layer_coverage_above_95_percent(self):
        """Domain layer doit avoir > 95% de coverage."""
        cov = coverage.Coverage()
        cov.load()
        
        domain_files = Path("src/boursa_vision/domain").rglob("*.py") 
        total_lines = 0
        covered_lines = 0
        
        for file in domain_files:
            analysis = cov.analysis2(str(file))
            total_lines += len(analysis[1]) + len(analysis[2]) 
            covered_lines += len(analysis[1])
            
        coverage_percent = (covered_lines / total_lines) * 100
        assert coverage_percent > 95, f"Domain coverage: {coverage_percent:.1f}%"
    
    def test_application_layer_coverage_above_90_percent(self):
        """Application layer doit avoir > 90% de coverage."""
        # Impl√©mentation similaire pour application/
        pass
    
    def test_infrastructure_layer_coverage_above_80_percent(self):
        """Infrastructure layer doit avoir > 80% de coverage."""
        # Impl√©mentation similaire pour infrastructure/
        pass
```

### Reporting et Dashboards

```python
# scripts/generate_test_report.py
"""G√©n√®re un rapport de test complet."""

import json
from pathlib import Path
import pytest
import coverage
from dataclasses import dataclass
from typing import Dict, List


@dataclass
class TestMetrics:
    """M√©triques de test par couche."""
    layer: str
    test_count: int
    coverage_percent: float
    avg_test_duration: float
    failed_tests: List[str]


def generate_comprehensive_report():
    """G√©n√®re un rapport de test complet."""
    
    # Ex√©cuter les tests avec m√©triques
    pytest_args = [
        "--json-report", 
        "--json-report-file=test_results.json",
        "--durations=0",
        "--cov=src",
        "--cov-report=json"
    ]
    
    pytest.main(pytest_args)
    
    # Analyser les r√©sultats
    with open("test_results.json") as f:
        test_results = json.load(f)
        
    with open("coverage.json") as f:
        coverage_results = json.load(f)
    
    # G√©n√©rer le rapport par couche
    layers = ["domain", "application", "infrastructure", "presentation"]
    metrics = []
    
    for layer in layers:
        layer_metrics = analyze_layer_metrics(layer, test_results, coverage_results)
        metrics.append(layer_metrics)
    
    # G√©n√©rer rapport HTML
    generate_html_report(metrics)
    
    # G√©n√©rer dashboard JSON pour int√©gration CI/CD
    generate_dashboard_data(metrics)


def analyze_layer_metrics(layer: str, test_results: dict, coverage_results: dict) -> TestMetrics:
    """Analyse les m√©triques pour une couche sp√©cifique."""
    
    # Filter tests by layer
    layer_tests = [
        test for test in test_results["tests"] 
        if f"/{layer}/" in test["nodeid"]
    ]
    
    # Calculate metrics
    test_count = len(layer_tests)
    failed_tests = [test["nodeid"] for test in layer_tests if test["outcome"] == "failed"]
    
    if test_count > 0:
        avg_duration = sum(test["duration"] for test in layer_tests) / test_count
    else:
        avg_duration = 0
    
    # Calculate coverage for layer
    layer_files = [
        file for file in coverage_results["files"] 
        if f"/{layer}/" in file
    ]
    
    if layer_files:
        total_coverage = sum(
            coverage_results["files"][file]["summary"]["percent_covered"] 
            for file in layer_files
        ) / len(layer_files)
    else:
        total_coverage = 0
    
    return TestMetrics(
        layer=layer,
        test_count=test_count,
        coverage_percent=total_coverage,
        avg_test_duration=avg_duration,
        failed_tests=failed_tests
    )
```

---

## üõ†Ô∏è Conventions et Standards

### Nomenclature des Tests

```python
# Convention de nommage descriptive
class TestPortfolioCreation:
    """Tests pour la cr√©ation de portefeuilles."""
    
    def test_should_create_portfolio_when_data_is_valid(self):
        """Should create portfolio when all required data is provided and valid."""
        pass
    
    def test_should_raise_validation_error_when_name_is_empty(self):
        """Should raise ValidationError when portfolio name is empty or None."""
        pass
    
    def test_should_assign_unique_id_when_portfolio_is_created(self):
        """Should assign a unique UUID when a new portfolio is successfully created."""
        pass


# Convention pour les tests param√©tr√©s
class TestPortfolioValidation:
    """Tests de validation pour les portefeuilles."""
    
    @pytest.mark.parametrize("invalid_name", [
        pytest.param("", id="empty_string"),
        pytest.param(None, id="none_value"), 
        pytest.param("   ", id="whitespace_only"),
        pytest.param("a" * 256, id="too_long"),
    ])
    def test_should_reject_invalid_portfolio_names(self, invalid_name):
        """Should reject portfolio creation when name is invalid."""
        pass
```

### Structure AAA (Arrange-Act-Assert)

```python
def test_portfolio_value_calculation_with_multiple_investments(
    self, 
    portfolio_factory,
    investment_factory
):
    """Should calculate correct total value when portfolio contains multiple investments."""
    
    # Arrange - Configuration du test
    portfolio = portfolio_factory.create(name="Tech Portfolio")
    
    apple_investment = investment_factory.create(
        symbol="AAPL", 
        quantity=10, 
        price=150.0
    )
    google_investment = investment_factory.create(
        symbol="GOOGL",
        quantity=5,
        price=2500.0
    )
    
    expected_total = (10 * 150.0) + (5 * 2500.0)
    
    # Act - Ex√©cution de l'action test√©e
    portfolio.add_investment(apple_investment)
    portfolio.add_investment(google_investment)
    
    actual_total = portfolio.calculate_total_value()
    
    # Assert - V√©rification des r√©sultats
    assert actual_total == expected_total
    assert len(portfolio.investments) == 2
    assert apple_investment in portfolio.investments
    assert google_investment in portfolio.investments
```

### Gestion des Erreurs et Edge Cases

```python
class TestPortfolioEdgeCases:
    """Tests des cas limites pour les portefeuilles."""
    
    def test_should_handle_empty_portfolio_gracefully(self, empty_portfolio):
        """Should handle operations on empty portfolio without errors."""
        # Test des op√©rations sur portfolio vide
        assert empty_portfolio.total_value == 0
        assert len(empty_portfolio.investments) == 0
        assert empty_portfolio.is_diversified() is False
    
    def test_should_handle_negative_investment_values(self, portfolio):
        """Should handle negative investment values correctly."""
        # Cas o√π un investissement perd de la valeur
        negative_investment = Investment(
            symbol="LOSS", 
            quantity=10, 
            purchase_price=100.0,
            current_price=50.0  # Perte de 50%
        )
        
        portfolio.add_investment(negative_investment)
        
        # V√©rifier que la perte est correctement calcul√©e
        assert portfolio.total_unrealized_gain < 0
        assert portfolio.total_value == 500.0  # 10 * 50
    
    @pytest.mark.parametrize("extreme_value", [
        pytest.param(float('inf'), id="infinite_value"),
        pytest.param(float('-inf'), id="negative_infinite"),
        pytest.param(float('nan'), id="not_a_number"),
        pytest.param(1e308, id="very_large_number"),
    ])
    def test_should_handle_extreme_numeric_values(self, extreme_value, portfolio):
        """Should handle extreme numeric values gracefully."""
        with pytest.raises(ValueError, match="Invalid numeric value"):
            investment = Investment(symbol="EXTREME", quantity=1, price=extreme_value)
            portfolio.add_investment(investment)
```

---

## üîÑ Migration des Tests Existants

### Plan de Migration en Phases

**Phase 1: Audit et Inventaire (Semaine 1)**
```python
# Script d'audit des tests existants
def audit_existing_tests():
    """Audit des tests existants pour planifier la migration."""
    
    test_files = Path("tests").rglob("test_*.py")
    audit_report = {
        "total_tests": 0,
        "by_category": {"unit": 0, "integration": 0, "e2e": 0, "unknown": 0},
        "by_status": {"passing": 0, "failing": 0, "ignored": 0},
        "migration_candidates": [],
        "deletion_candidates": []
    }
    
    for test_file in test_files:
        # Analyser chaque fichier de test
        tests_in_file = analyze_test_file(test_file)
        
        for test in tests_in_file:
            audit_report["total_tests"] += 1
            
            # Cat√©goriser le test
            category = categorize_test(test)
            audit_report["by_category"][category] += 1
            
            # √âvaluer la qualit√©
            if is_migration_candidate(test):
                audit_report["migration_candidates"].append(test)
            elif should_be_deleted(test):
                audit_report["deletion_candidates"].append(test)
    
    generate_audit_report(audit_report)
    return audit_report
```

**Phase 2: Migration par Couches (Semaines 2-4)**

```python
# Exemples de migration de tests existants

# AVANT: Test ancien sans DI
def test_portfolio_creation_old():
    """Test ancien avec d√©pendances hardcod√©es."""
    
    # Probl√®mes: hardcoded DB, pas d'isolation, lent
    db = PostgresDatabase(url="postgresql://...")
    repo = PortfolioRepository(db)
    
    portfolio_data = {"name": "Test Portfolio"}
    portfolio = repo.create(portfolio_data)
    
    assert portfolio.name == "Test Portfolio"


# APR√àS: Test modernis√© avec DI  
@pytest.mark.unit
def test_portfolio_creation_new(unit_container, portfolio_factory):
    """Should create portfolio when valid data is provided."""
    
    # Arrange - DI container avec mocks
    use_case = unit_container.application.create_portfolio_use_case()
    portfolio_data = portfolio_factory.build_data()
    
    # Act
    result = use_case.execute(portfolio_data)
    
    # Assert
    assert result.success
    assert result.portfolio.name == portfolio_data["name"]
```

**Phase 3: Optimisation et Standardisation (Semaine 5)**

```python
# Refactoring des fixtures communes
@pytest.fixture(scope="session", autouse=True)
def setup_test_environment():
    """Configuration automatique de l'environnement de test."""
    
    # Configuration des logs en mode test
    import logging
    logging.getLogger("boursa_vision").setLevel(logging.WARNING)
    
    # D√©sactiver les connexions externes
    import os
    os.environ["YFINANCE_MOCK_MODE"] = "true"
    os.environ["REDIS_MOCK_MODE"] = "true"
    
    yield
    
    # Cleanup global
    cleanup_test_artifacts()


def cleanup_test_artifacts():
    """Nettoie les artefacts de test."""
    
    # Supprimer les fichiers temporaires
    import tempfile
    import shutil
    
    temp_dirs = ["/tmp/boursa_test_*", "/tmp/pytest-*"]
    for pattern in temp_dirs:
        for path in glob.glob(pattern):
            shutil.rmtree(path, ignore_errors=True)
```

### Checklist de Migration

- [ ] **Audit Initial**
  - [ ] Inventaire des tests existants 
  - [ ] Identification des tests obsol√®tes
  - [ ] Analyse des d√©pendances hardcod√©es

- [ ] **Configuration**
  - [ ] Mise √† jour pytest.ini
  - [ ] Configuration des markers
  - [ ] Setup des fixtures DI

- [ ] **Migration des Tests**
  - [ ] Tests Domain (priorit√© haute)
  - [ ] Tests Application (priorit√© haute) 
  - [ ] Tests Infrastructure (priorit√© moyenne)
  - [ ] Tests Presentation (priorit√© basse)

- [ ] **Validation**
  - [ ] Coverage par couche respect√©
  - [ ] Performance des tests < seuils
  - [ ] Stabilit√© (pas de flaky tests)

- [ ] **Documentation**
  - [ ] Guide des fixtures
  - [ ] Exemples par type de test
  - [ ] Conventions d'√©quipe

---

## üìà M√©triques et KPIs de R√©ussite

### Objectifs Quantitatifs

| M√©trique | Objectif | Actuel | Status |
|----------|----------|---------|--------|
| **Test Coverage** | > 85% | 73% | üîÑ En cours |
| **Domain Coverage** | > 95% | - | ‚è≥ √Ä mesurer |
| **Test Speed** | < 2min suite compl√®te | ~5min | üîÑ En cours |  
| **Unit Test Speed** | < 100ms par test | - | ‚è≥ √Ä mesurer |
| **Flaky Tests** | 0% | - | ‚è≥ √Ä mesurer |
| **Test Maintenance** | < 10% effort dev | - | ‚è≥ √Ä mesurer |

### Monitoring et Alertes

```python
# tests/monitoring/test_health_metrics.py
"""Tests de monitoring de la sant√© de la suite de test."""

import time
import pytest
from pathlib import Path


class TestSuiteHealthMetrics:
    """M√©triques de sant√© de la suite de test."""
    
    def test_unit_tests_execution_time_under_threshold(self):
        """Les tests unitaires doivent s'ex√©cuter en moins de 100ms chacun."""
        
        start_time = time.time()
        
        # Ex√©cuter tous les tests unitaires
        result = pytest.main(["-m", "unit", "--tb=no", "-q"])
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Compter le nombre de tests unitaires
        unit_test_count = count_tests_with_marker("unit")
        
        if unit_test_count > 0:
            avg_time_per_test = (execution_time / unit_test_count) * 1000  # en ms
            assert avg_time_per_test < 100, f"Tests unitaires trop lents: {avg_time_per_test:.1f}ms en moyenne"
    
    def test_no_ignored_tests_in_production_code(self):
        """Aucun test ne doit √™tre ignor√© dans le code de production."""
        
        test_files = Path("tests").rglob("*.py")
        ignored_tests = []
        
        for test_file in test_files:
            content = test_file.read_text()
            
            # Chercher les patterns d'ignore
            ignore_patterns = ["@pytest.mark.skip", "pytest.skip(", "# TODO: fix"]
            
            for pattern in ignore_patterns:
                if pattern in content:
                    ignored_tests.append(str(test_file))
                    
        assert len(ignored_tests) == 0, f"Tests ignor√©s trouv√©s: {ignored_tests}"
    
    def test_test_coverage_regression_protection(self):
        """Protection contre la r√©gression du coverage."""
        
        import coverage
        cov = coverage.Coverage()
        cov.load()
        
        current_coverage = cov.report(show_missing=False)
        minimum_coverage = 85.0  # Seuil minimum
        
        assert current_coverage >= minimum_coverage, \
            f"Coverage en r√©gression: {current_coverage:.1f}% < {minimum_coverage}%"
```

---

## üéØ Conclusion

Ce guide √©tablit les fondations d'une architecture de test moderne, robuste et maintenable pour BoursaVision. Les principes √©nonc√©s garantissent :

- **üöÄ Performance**: Tests rapides avec feedback imm√©diat
- **üîí Fiabilit√©**: Tests d√©terministes et isol√©s  
- **üß† Intelligibilit√©**: Tests expressifs et document√©s
- **üîß Maintenabilit√©**: Architecture √©volutive avec le produit
- **‚ö° Productivity**: Outils et conventions qui acc√©l√®rent le d√©veloppement

**Prochaines √©tapes:**
1. ‚úÖ Appliquer les conventions aux nouveaux tests
2. üîÑ Migrer progressivement les tests existants  
3. üìä Mettre en place le monitoring des m√©triques
4. üéì Former l'√©quipe aux nouveaux standards

---

*Guide maintenu par l'√©quipe BoursaVision ‚Ä¢ Derni√®re mise √† jour: D√©cembre 2024*
